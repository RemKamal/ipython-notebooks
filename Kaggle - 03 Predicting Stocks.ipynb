{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "9/22/2013\n",
      "# Kaggle Competition - Predicting relative stock price\n",
      "\n",
      "Deadline: 10/1/2013\n",
      "State of the art error: 0.40458\n",
      "\n",
      "For this competition, you are asked to predict the percentage change in a financial instrument at a time 2 hours in the future.  The data represents features of various financial securities (198 in total) recorded at 5-minute intervals throughout a trading day.  To discourage cheating, you are not provided with the features' names or the specific dates.\n",
      "\n",
      "__data.zip__ - contains features for 510 days worth of trading, including 200 training days and 310 testing days  \n",
      "__trainLabels.csv__ - contains the targets for the 200 training days  \n",
      "__sampleSubmission.csv__ - shows the submission format\n",
      "\n",
      "Each variable named O1, O2, O3, etc. (the outputs) represents a percent change in the value of a security.  Each variable named I1, I2, I3, etc. (the inputs) represents a feature. The underlying securities and features represented by these anonymized names are the same across all files (e.g. O1 will always be the same stock).\n",
      "\n",
      "Within each trading day, you are provided the outputs as a relative percentage compared to the previous day's closing price.  The first line of each data file represents the previous close. For example, if a security closed at \\$1 the previous day and opened at \\$2 the next day, the first output would be 0, then 100.  All output values are computed relative to the previous day's close. The timestamps within each file are as follows (ignoring the header row):\n",
      "\n",
      "Line 1 = Outputs and inputs at previous day's close (4PM ET)  \n",
      "Line 2 = Outputs and inputs at current day's open (9:30AM ET)  \n",
      "Line 3 = Outputs and inputs at 9:35AM ET  \n",
      "...  \n",
      "Line 55 = Outputs and inputs at 1:55PM ET\n",
      "\n",
      "You are asked to predict the outputs 2 hours later, at 4PM ET.\n",
      "\n",
      "Evaluation is by the mean absolute error, $MAE = \\frac{1}{n} \\sum_{i = 1}^n |y_i - \\hat{y}_i |$\n",
      "\n",
      "---\n",
      "###Results\n",
      "- Last-observed benchmark is surprisingly good; achieves private score of 0.44 and public score of 0.42007\n",
      "- An increase in the private performance doesn't necessarily correspond to a better public score, for instance  \n",
      "    - Exponential filter with 0.5 discount rate gets a better private score than 0.9 discount rate, but a worse public score\n",
      "- Using a linear model (per stock) of last observed price and the direction (derivative) doesn't improve public score\n",
      "    - This could just be because I'm improperly optimizing the coefficients\n",
      "- My best result so far is $x_{i,t} = a_i x_{i,t-1} + b_i$ where $x_{i,t-1}$ is the last observed price and where $i$ is the $i^{\\rm th}$ stock\n",
      "\n",
      "\n",
      "###To-do\n",
      "- I'm using handwritten gradient descent, but could probably use scipy optimize for better results\n",
      "- Actually use features\n",
      "- Try random forests\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Pre-Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Change directory and import modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /home/lane/Kaggle/03\\ Predicting\\ Stock\\ Prices/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/lane/Kaggle/03 Predicting Stock Prices\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[0m\u001b[01;34mData\u001b[0m/          infoTheory.pyc  linearMAE.pyc      makeSubmission_temp.py  meanAbsoluteError.pyc  scraps.py\r\n",
        "infoTheory.py  linearMAE.py    makeSubmission.py  meanAbsoluteError.py    mh_test.py\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import cross_validation\n",
      "from scipy import optimize\n",
      "import meanAbsoluteError as err\n",
      "import numpy as np\n",
      "import datetime\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Import and format data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#DATA PROCESSING\n",
      "#create the training & test sets, skipping the header row with [1:]\n",
      "trainingDays = range(1,511) #200 days of training data, 510 total days\n",
      "headers = np.genfromtxt(open('Data/data/1.csv','r'),delimiter=',',dtype='string')[0]\n",
      "numRows = (np.array(np.genfromtxt(open('Data/data/1.csv','r'),delimiter=',',dtype='f8')[1:])).shape[0]\n",
      "numCols = (np.array(np.genfromtxt(open('Data/data/1.csv','r'),delimiter=',',dtype='f8')[1:])).shape[1]\n",
      "isOutput = [headers[x][0] == 'O' for x in range(0,numCols)]\n",
      "isInput  = [headers[x][0] == 'I' for x in range(0,numCols)]\n",
      "trainOutput = np.zeros((len(trainingDays),numRows,sum(isOutput)))\n",
      "trainInput  = np.zeros((len(trainingDays),numRows,sum(isInput)))\n",
      "for i in trainingDays:\n",
      "    dataset = np.genfromtxt(open('Data/data/'+str(i)+'.csv','r'), delimiter=',', dtype='f8')[1:]\n",
      "    dataset = np.array(dataset)  # (5minIncrement,stock/feature)\n",
      "    for j in range(0,numCols):\n",
      "        if headers[j][0] == 'O':\n",
      "            trainOutput[i-1,:,j] = dataset[:,j]   # (day,5minIncrement,stock)\n",
      "        elif headers[j][0] == 'I':\n",
      "            trainInput[i-1,:,(j-sum(isOutput))] = dataset[:,j]    # (day,5minIncrement,feature)\n",
      "\n",
      "#target prices 2 hours later (only outputs, no inputs) \n",
      "target = np.array(np.genfromtxt(open('Data/trainLabels.csv','r'), delimiter=',', dtype='f8')[1:])\n",
      "target = target[:,1:]  # (day,price2HrsLater)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-f9d6f029d75e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrainInput\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingDays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainingDays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/data/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (5minIncrement,stock/feature)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumCols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skiprows, skip_header, skip_footer, converters, missing, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m         rows = zip(*[map(converter._loose_call, map(itemgetter(i), rows))\n\u001b[1;32m-> 1661\u001b[1;33m                      for (i, converter) in enumerate(converters)])\n\u001b[0m\u001b[0;32m   1662\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m         rows = zip(*[map(converter._strict_call, map(itemgetter(i), rows))\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/_iotools.pyc\u001b[0m in \u001b[0;36m_loose_call\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_loose_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Linear Models\n",
      "\n",
      "---\n",
      "First with hand-built gradient method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize variables\n",
      "iterations   = 100\n",
      "learningRate = 0.01\n",
      "h            = (2e-15)**(1./3)\n",
      "coeff        = np.zeros((sum(isOutput),3))\n",
      "coeff[:,1]   = np.squeeze(np.ones((sum(isOutput),1)))\n",
      "error        = []\n",
      "gradient     = [0,0,0]\n",
      "pred0P = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred0M = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred1P = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred1M = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred2P = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred2M = np.zeros((len(trainingDays),sum(isOutput)))\n",
      "pred   = np.zeros((len(trainingDays),sum(isOutput)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The following model achives state-of-the art performance: 0.41947\n",
      "\n",
      "---\n",
      "Linear model on last-observed value and last derivative, with coefficients that vary by stock.  Coefficients are optimized by hand-created gradient using 100 iterations for each stock and day"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run model\n",
      "for stock in range(0,sum(isOutput)):\n",
      "    for i in range(0,iterations):\n",
      "        for day in range(0,len(trainingDays)):\n",
      "            deriv = trainOutput[day,-1,stock] - trainOutput[day,-2,stock]\n",
      "            pred0P[day,stock] = (coeff[stock,1])*trainOutput[day,-1,stock] + (coeff[stock,2])*deriv + (coeff[stock,0] + h)\n",
      "            pred0M[day,stock] = (coeff[stock,1])*trainOutput[day,-1,stock] + (coeff[stock,2])*deriv + (coeff[stock,0] - h)\n",
      "            pred1P[day,stock] = (coeff[stock,1]+h)*trainOutput[day,-1,stock] +  (coeff[stock,2])*deriv + (coeff[stock,0])\n",
      "            pred1M[day,stock] = (coeff[stock,1]-h)*trainOutput[day,-1,stock] +  (coeff[stock,2])*deriv +(coeff[stock,0])\n",
      "            pred2P[day,stock] = (coeff[stock,1])*trainOutput[day,-1,stock] +  (coeff[stock,2]+h)*deriv + (coeff[stock,0])\n",
      "            pred2M[day,stock] = (coeff[stock,1])*trainOutput[day,-1,stock] +  (coeff[stock,2]-h)*deriv +(coeff[stock,0])\n",
      "            pred[day,stock]   = (coeff[stock,1])*trainOutput[day,-1,stock] + (coeff[stock,2])*deriv +(coeff[stock,0])\n",
      "\n",
      "        gradient[0] = (err.maeFun(target,pred0P[0:200,:]) - err.maeFun(target,pred0M[0:200,:]))/(2*h)\n",
      "        gradient[1] = (err.maeFun(target,pred1P[0:200,:]) - err.maeFun(target,pred1M[0:200,:]))/(2*h)\n",
      "        gradient[2] = (err.maeFun(target,pred2P[0:200,:]) - err.maeFun(target,pred2M[0:200,:]))/(2*h)\n",
      "        coeff[stock,:] = [coeff[stock,x] - learningRate*gradient[x] for x in range(0,len(gradient))]\n",
      "        #print \"Stock: \" + str(stock) + \" Error: \" + str(err.maeFun(target[:,stock],pred[0:200,stock])) + \" Coeff: \" + str(coeff[stock,:])\n",
      "\n",
      "    error.append(err.maeFun(target,pred[0:200,:])) #maeFun(actual,pred)\n",
      "    print \"Results: \" + str(error[-1]) + \" Variance of coefficients: \" + str([np.var(coeff[:,0]), np.var(coeff[:,1])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plot(error)\n",
      "ax = pylab.axes()\n",
      "ax.set_title('Gradient Descent on Coefficients')\n",
      "ax.set_xlabel('Error')\n",
      "ax.set_ylabel('Stocks')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Save predictions to file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.savetxt('Data/submission'+str(datetime.date.today())+'.csv', pred[200:510,:], delimiter=',', fmt='%f')  #predictions for file 201 to 510"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Linear Model on all datapoints with scipy.optimize\n",
      "\n",
      "Define model and use scipy optimize to see if you get better results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def linearModel(coeff,trainOutput):\n",
      "    \n",
      "    # Set constants and initialize vars\n",
      "    numTrainingDays = trainOutput.shape[0]  #510\n",
      "    numTimes        = trainOutput.shape[1]  #55\n",
      "    numStocks       = trainOutput.shape[2]  #198\n",
      "    pred            = np.zeros((numTrainingDays,numStocks))\n",
      "    \n",
      "    # need coeff in format (numStocks, numTimes)\n",
      "    # scipy.optimize.minimize reshapes coeff as vector\n",
      "    timesUsed = 2  # could be numTimes\n",
      "    coeff = coeff.reshape((numStocks,timesUsed))\n",
      "    \n",
      "    # Run model\n",
      "    for stock in xrange(numStocks):\n",
      "        for day in xrange(numTrainingDays):\n",
      "            pred[day,stock] = np.dot(coeff[stock,-timesUsed:-1],trainOutput[day,-timesUsed:-1,stock])\n",
      "    \n",
      "    return pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def errorLinear(coeff, trainOutput, target):\n",
      "    # function of coeff, trainOutput, & target\n",
      "    \n",
      "    pred = linearModel(coeff,trainOutput)\n",
      "    \n",
      "    return err.maeFun(target,pred[0:200,:]) #maeFun(actual,pred)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Optimize using scipy.optimize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# intial parameters\n",
      "timesUsed   = 2\n",
      "coeff       = np.zeros((trainOutput.shape[2],timesUsed)) # (numStocks, timesUsed), where timesUsed could = trainOutput.shape[1]\n",
      "coeff[:,-1] = np.squeeze(np.ones((trainOutput.shape[2],1)))\n",
      "res         = optimize.minimize(errorLinear, x0=coeff, args=[trainOutput,target], method='nelder-mead', options={'xtol': 1e-8, 'disp': True, 'maxiter' : 5})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "too many indices",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-93-341170670e06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcoeff\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrorLinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nelder-mead'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'xtol'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'disp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maxiter'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nelder-mead'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'powell'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_powell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[1;34m(func, x0, args, callback, xtol, ftol, maxiter, maxfev, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[0mallvecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m     \u001b[0mfsim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m     \u001b[0mnonzdelt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mzdelt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00025\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-92-467a598c09e1>\u001b[0m in \u001b[0;36merrorLinear\u001b[1;34m(coeff, trainOutput, target)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# function of coeff, trainOutput, & target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinearModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaeFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#maeFun(actual,pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-91-40de7082f0f6>\u001b[0m in \u001b[0;36mlinearModel\u001b[1;34m(coeff, trainOutput)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumTrainingDays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstock\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: too many indices"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, (10890,), (510, 55, 198)]\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opt_coeff    = res.x\n",
      "\n",
      "coeff        = np.zeros((trainOutput.shape[2],trainOutput.shape[1]))\n",
      "coeff[:,1]   = np.squeeze(np.ones((sum(isOutput),1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numTrainingDays = trainOutput.shape[0]  #510\n",
      "numTimes        = trainOutput.shape[1]  #55\n",
      "numStocks       = trainOutput.shape[2]  #198\n",
      "pred            = np.zeros((numTrainingDays,numStocks))\n",
      "    \n",
      "# Run model\n",
      "for stock in xrange(numStocks):\n",
      "    for day in xrange(numTrainingDays):\n",
      "        pred[day,stock] = np.dot(coeff[stock,:],trainOutput[day,:,stock])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(coeff[0,:])\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD9CAYAAABDaefJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwAPoagDgIf8H9FP5lUS9TlqAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3d72a10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD9CAYAAABDaefJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwAPoagDgIf8H9FP5lUS9TlqAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x3cf91d0>"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stock"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "197"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "day"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "509"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yoda = np.dot(coeff[stock,:],trainOutput[day,:,stock])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yoda.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "()"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.dot(coeff[stock,:],trainOutput[day,:,stock])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "-1.03"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numTrainingDays = trainOutput.shape[0]  #510\n",
      "numTimes        = trainOutput.shape[1]  #55\n",
      "numStocks       = trainOutput.shape[2]  #198\n",
      "for days in xrange(numTrainingDays):\n",
      "    print days"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "5\n",
        "6\n",
        "7\n",
        "8\n",
        "9\n",
        "10\n",
        "11\n",
        "12\n",
        "13\n",
        "14\n",
        "15\n",
        "16\n",
        "17\n",
        "18\n",
        "19\n",
        "20\n",
        "21\n",
        "22\n",
        "23\n",
        "24\n",
        "25\n",
        "26\n",
        "27\n",
        "28\n",
        "29\n",
        "30\n",
        "31\n",
        "32\n",
        "33\n",
        "34\n",
        "35\n",
        "36\n",
        "37\n",
        "38\n",
        "39\n",
        "40\n",
        "41\n",
        "42\n",
        "43\n",
        "44\n",
        "45\n",
        "46\n",
        "47\n",
        "48\n",
        "49\n",
        "50\n",
        "51\n",
        "52\n",
        "53\n",
        "54\n",
        "55\n",
        "56\n",
        "57\n",
        "58\n",
        "59\n",
        "60\n",
        "61\n",
        "62\n",
        "63\n",
        "64\n",
        "65\n",
        "66\n",
        "67\n",
        "68\n",
        "69\n",
        "70\n",
        "71\n",
        "72\n",
        "73\n",
        "74\n",
        "75\n",
        "76\n",
        "77\n",
        "78\n",
        "79\n",
        "80\n",
        "81\n",
        "82\n",
        "83\n",
        "84\n",
        "85\n",
        "86\n",
        "87\n",
        "88\n",
        "89\n",
        "90\n",
        "91\n",
        "92\n",
        "93\n",
        "94\n",
        "95\n",
        "96\n",
        "97\n",
        "98\n",
        "99\n",
        "100\n",
        "101\n",
        "102\n",
        "103\n",
        "104\n",
        "105\n",
        "106\n",
        "107\n",
        "108\n",
        "109\n",
        "110\n",
        "111\n",
        "112\n",
        "113\n",
        "114\n",
        "115\n",
        "116\n",
        "117\n",
        "118\n",
        "119\n",
        "120\n",
        "121\n",
        "122\n",
        "123\n",
        "124\n",
        "125\n",
        "126\n",
        "127\n",
        "128\n",
        "129\n",
        "130\n",
        "131\n",
        "132\n",
        "133\n",
        "134\n",
        "135\n",
        "136\n",
        "137\n",
        "138\n",
        "139\n",
        "140\n",
        "141\n",
        "142\n",
        "143\n",
        "144\n",
        "145\n",
        "146\n",
        "147\n",
        "148\n",
        "149\n",
        "150\n",
        "151\n",
        "152\n",
        "153\n",
        "154\n",
        "155\n",
        "156\n",
        "157\n",
        "158\n",
        "159\n",
        "160\n",
        "161\n",
        "162\n",
        "163\n",
        "164\n",
        "165\n",
        "166\n",
        "167\n",
        "168\n",
        "169\n",
        "170\n",
        "171\n",
        "172\n",
        "173\n",
        "174\n",
        "175\n",
        "176\n",
        "177\n",
        "178\n",
        "179\n",
        "180\n",
        "181\n",
        "182\n",
        "183\n",
        "184\n",
        "185\n",
        "186\n",
        "187\n",
        "188\n",
        "189\n",
        "190\n",
        "191\n",
        "192\n",
        "193\n",
        "194\n",
        "195\n",
        "196\n",
        "197\n",
        "198\n",
        "199\n",
        "200\n",
        "201\n",
        "202\n",
        "203\n",
        "204\n",
        "205\n",
        "206\n",
        "207\n",
        "208\n",
        "209\n",
        "210\n",
        "211\n",
        "212\n",
        "213\n",
        "214\n",
        "215\n",
        "216\n",
        "217\n",
        "218\n",
        "219\n",
        "220\n",
        "221\n",
        "222\n",
        "223\n",
        "224\n",
        "225\n",
        "226\n",
        "227\n",
        "228\n",
        "229\n",
        "230\n",
        "231\n",
        "232\n",
        "233\n",
        "234\n",
        "235\n",
        "236\n",
        "237\n",
        "238\n",
        "239\n",
        "240\n",
        "241\n",
        "242\n",
        "243\n",
        "244\n",
        "245\n",
        "246\n",
        "247\n",
        "248\n",
        "249\n",
        "250\n",
        "251\n",
        "252\n",
        "253\n",
        "254\n",
        "255\n",
        "256\n",
        "257\n",
        "258\n",
        "259\n",
        "260\n",
        "261\n",
        "262\n",
        "263\n",
        "264\n",
        "265\n",
        "266\n",
        "267\n",
        "268\n",
        "269\n",
        "270\n",
        "271\n",
        "272\n",
        "273\n",
        "274\n",
        "275\n",
        "276\n",
        "277\n",
        "278\n",
        "279\n",
        "280\n",
        "281\n",
        "282\n",
        "283\n",
        "284\n",
        "285\n",
        "286\n",
        "287\n",
        "288\n",
        "289\n",
        "290\n",
        "291\n",
        "292\n",
        "293\n",
        "294\n",
        "295\n",
        "296\n",
        "297\n",
        "298\n",
        "299\n",
        "300\n",
        "301\n",
        "302\n",
        "303\n",
        "304\n",
        "305\n",
        "306\n",
        "307\n",
        "308\n",
        "309\n",
        "310\n",
        "311\n",
        "312\n",
        "313\n",
        "314\n",
        "315\n",
        "316\n",
        "317\n",
        "318\n",
        "319\n",
        "320\n",
        "321\n",
        "322\n",
        "323\n",
        "324\n",
        "325\n",
        "326\n",
        "327\n",
        "328\n",
        "329\n",
        "330\n",
        "331\n",
        "332\n",
        "333\n",
        "334\n",
        "335\n",
        "336\n",
        "337\n",
        "338\n",
        "339\n",
        "340\n",
        "341\n",
        "342\n",
        "343\n",
        "344\n",
        "345\n",
        "346\n",
        "347\n",
        "348\n",
        "349\n",
        "350\n",
        "351\n",
        "352\n",
        "353\n",
        "354\n",
        "355\n",
        "356\n",
        "357\n",
        "358\n",
        "359\n",
        "360\n",
        "361\n",
        "362\n",
        "363\n",
        "364\n",
        "365\n",
        "366\n",
        "367\n",
        "368\n",
        "369\n",
        "370\n",
        "371\n",
        "372\n",
        "373\n",
        "374\n",
        "375\n",
        "376\n",
        "377\n",
        "378\n",
        "379\n",
        "380\n",
        "381\n",
        "382\n",
        "383\n",
        "384\n",
        "385\n",
        "386\n",
        "387\n",
        "388\n",
        "389\n",
        "390\n",
        "391\n",
        "392\n",
        "393\n",
        "394\n",
        "395\n",
        "396\n",
        "397\n",
        "398\n",
        "399\n",
        "400\n",
        "401\n",
        "402\n",
        "403\n",
        "404\n",
        "405\n",
        "406\n",
        "407\n",
        "408\n",
        "409\n",
        "410\n",
        "411\n",
        "412\n",
        "413\n",
        "414\n",
        "415\n",
        "416\n",
        "417\n",
        "418\n",
        "419\n",
        "420\n",
        "421\n",
        "422\n",
        "423\n",
        "424\n",
        "425\n",
        "426\n",
        "427\n",
        "428\n",
        "429\n",
        "430\n",
        "431\n",
        "432\n",
        "433\n",
        "434\n",
        "435\n",
        "436\n",
        "437\n",
        "438\n",
        "439\n",
        "440\n",
        "441\n",
        "442\n",
        "443\n",
        "444\n",
        "445\n",
        "446\n",
        "447\n",
        "448\n",
        "449\n",
        "450\n",
        "451\n",
        "452\n",
        "453\n",
        "454\n",
        "455\n",
        "456\n",
        "457\n",
        "458\n",
        "459\n",
        "460\n",
        "461\n",
        "462\n",
        "463\n",
        "464\n",
        "465\n",
        "466\n",
        "467\n",
        "468\n",
        "469\n",
        "470\n",
        "471\n",
        "472\n",
        "473\n",
        "474\n",
        "475\n",
        "476\n",
        "477\n",
        "478\n",
        "479\n",
        "480\n",
        "481\n",
        "482\n",
        "483\n",
        "484\n",
        "485\n",
        "486\n",
        "487\n",
        "488\n",
        "489\n",
        "490\n",
        "491\n",
        "492\n",
        "493\n",
        "494\n",
        "495\n",
        "496\n",
        "497\n",
        "498\n",
        "499\n",
        "500\n",
        "501\n",
        "502\n",
        "503\n",
        "504\n",
        "505\n",
        "506\n",
        "507\n",
        "508\n",
        "509\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for stock in xrange(numStocks):\n",
      "        for day in xrange(numTrainingDays):\n",
      "            pred[day,stock] = np.dot(coeff[stock,:],trainOutput[day,:,stock])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "(510, 198)"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainOutput.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "(510, 55, 198)"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "timesUsed = 2\n",
      "-timesUsed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "-2"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}